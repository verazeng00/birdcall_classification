{"cells":[{"cell_type":"markdown","metadata":{},"source":["## About\n","\n","I've spent several days to make a successful submission and finally got a way to do that after 3 days of struggle. \n","I want Kaggle competitors to feel easy to participate in this competition, therefore I decided to share my Notebook.\n","\n","I would like to thank [@radek1](https://www.kaggle.com/radek1) for creating [a good starter notebook](https://www.kaggle.com/c/birdsong-recognition/discussion/160222), [@shonenkov](https://www.kaggle.com/shonenkov) for [a nice checking dataset and notebook](https://www.kaggle.com/shonenkov/sample-submission-using-custom-check) and several discussions to make this competition better, [@cwthompson](https://www.kaggle.com/cwthompson) for [showing the way to submit](https://www.kaggle.com/cwthompson/birdsong-making-a-prediction) using `test_audio`.\n","\n","I also would like to thank [@stefankahl](https://www.kaggle.com/stefankahl), [@tomdenton](https://www.kaggle.com/tomdenton), [@sohier](https://www.kaggle.com/sohier) for hosting a really interesting competition."]},{"cell_type":"markdown","metadata":{},"source":["In this notebook I tried to make submission using ResNet based model trained with log melspectrogram. I will create a notebook to show the way I trained the model but here I briefly describe my approach.\n","\n","* Randomly crop 5 seconds for each train audio clip each epoch.\n","* No augmentation.\n","* Use pretrained weight of `torchvision.models.resnet50`.\n","* Used `BCELoss`.\n","* Trained 100 epoch and used the weight which got best F1 (at 92epoch).\n","* `Adam` optimizer (`lr=0.001`) with `CosineAnnealingLR` (`T_max=10`).\n","* Use `StratifiedKFold(n_splits=5)` to split dataset and used only first fold\n","\n","Here are the parameter details.\n","\n","* `batch_size`: 100 (on V100, took 2 ~ 3hrs to run 100epochs)\n","* melspectrogram parameters\n","  - `n_mels`: 128\n","  - `fmin`: 20\n","  - `fmax`: 16000\n","* image size: 224 x 541 (I don't remember the exact width)"]},{"cell_type":"markdown","metadata":{},"source":["### Future direction\n","\n","There are a lot many to do to make improvement. It was a big challenge for me to make successful submission with very few feedback signal (like `Submission CSV Not Found` or `Notebook Exceeded Allowed Compute`), but this is just a beginning of the real challenge.\n","As described in https://www.kaggle.com/c/birdsong-recognition/discussion/160222#895234 , data augmentation is a key. I worked on [Freesound Audio Tagging 2019](https://www.kaggle.com/c/freesound-audio-tagging-2019) last year, which was also an audio competition (which is comparatively rare in Kaggle), and at that time data augmentation like pitch shift or reverb effect gave us a boost. This competition is not about environmental sound but about bird song, therefore we need to check what augmentation works best on this data by experiment. Maybe we can get a boost with different augmentation for different audio class.\n","\n","Mixup / BClearning or mixing different audio class may give us a rise I believe, since the test set has multiple sounds in the clip whereas train set has basically one class for one clip (of course we can use background sound information to treat train set as multilabel problem).\n","\n","Training procedure also has an important role, whether we use a procedure for multilabel problem (by using background sound) or for multiclass problem. The challenge of this competition can also be treated as *Domain Adaptation* problem, so we can use techniques for that.\n","\n","Model selection is also important, deeper model may give us a rise, but from my experience, *too deep* model are sometimes defeated by shallower model in audio classification."]},{"cell_type":"markdown","metadata":{},"source":["## Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-12-02T05:52:24.948121Z","iopub.status.busy":"2022-12-02T05:52:24.947712Z","iopub.status.idle":"2022-12-02T05:52:24.955877Z","shell.execute_reply":"2022-12-02T05:52:24.954592Z","shell.execute_reply.started":"2022-12-02T05:52:24.948082Z"},"trusted":true},"outputs":[],"source":["import cv2\n","import audioread\n","import logging\n","import os\n","import random\n","import time\n","import warnings\n","\n","import librosa\n","import numpy as np\n","import pandas as pd\n","import soundfile as sf\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.data as data\n","\n","from contextlib import contextmanager\n","from pathlib import Path\n","from typing import Optional\n","\n","from fastprogress import progress_bar\n","from sklearn.metrics import f1_score\n","from torchvision import models"]},{"cell_type":"markdown","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"source":["## Utilities"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T05:52:24.962051Z","iopub.status.busy":"2022-12-02T05:52:24.961505Z","iopub.status.idle":"2022-12-02T05:52:24.975788Z","shell.execute_reply":"2022-12-02T05:52:24.974587Z","shell.execute_reply.started":"2022-12-02T05:52:24.962012Z"},"trusted":true},"outputs":[],"source":["def set_seed(seed: int = 42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)  # type: ignore\n","    torch.backends.cudnn.deterministic = True  # type: ignore\n","    torch.backends.cudnn.benchmark = True  # type: ignore\n","    \n","    \n","def get_logger(out_file=None):\n","    logger = logging.getLogger()\n","    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n","    logger.handlers = []\n","    logger.setLevel(logging.INFO)\n","\n","    handler = logging.StreamHandler()\n","    handler.setFormatter(formatter)\n","    handler.setLevel(logging.INFO)\n","    logger.addHandler(handler)\n","\n","    if out_file is not None:\n","        fh = logging.FileHandler(out_file)\n","        fh.setFormatter(formatter)\n","        fh.setLevel(logging.INFO)\n","        logger.addHandler(fh)\n","    logger.info(\"logger set up\")\n","    return logger\n","    \n","    \n","@contextmanager\n","def timer(name: str, logger: Optional[logging.Logger] = None):\n","    t0 = time.time()\n","    msg = f\"[{name}] start\"\n","    if logger is None:\n","        print(msg)\n","    else:\n","        logger.info(msg)\n","    yield\n","\n","    msg = f\"[{name}] done in {time.time() - t0:.2f} s\"\n","    if logger is None:\n","        print(msg)\n","    else:\n","        logger.info(msg)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T05:52:24.979113Z","iopub.status.busy":"2022-12-02T05:52:24.978399Z","iopub.status.idle":"2022-12-02T05:52:24.996841Z","shell.execute_reply":"2022-12-02T05:52:24.994245Z","shell.execute_reply.started":"2022-12-02T05:52:24.979024Z"},"trusted":true},"outputs":[],"source":["logger = get_logger(\"main.log\")\n","set_seed(1213)"]},{"cell_type":"markdown","metadata":{},"source":["## Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T05:52:24.999919Z","iopub.status.busy":"2022-12-02T05:52:24.999595Z","iopub.status.idle":"2022-12-02T05:52:25.007247Z","shell.execute_reply":"2022-12-02T05:52:25.005978Z","shell.execute_reply.started":"2022-12-02T05:52:24.999889Z"},"trusted":true},"outputs":[],"source":["TEST = Path(\"../input/birdsong-recognition/test_audio\").exists()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T05:52:25.010275Z","iopub.status.busy":"2022-12-02T05:52:25.009433Z","iopub.status.idle":"2022-12-02T05:52:25.058395Z","shell.execute_reply":"2022-12-02T05:52:25.057639Z","shell.execute_reply.started":"2022-12-02T05:52:25.010228Z"},"trusted":true},"outputs":[],"source":["if TEST:\n","    DATA_DIR = Path(\"../input/birdsong-recognition/\")\n","else:\n","    # dataset created by @shonenkov, thanks!\n","    DATA_DIR = Path(\"../input/birdcall-check/\")\n","    \n","\n","test = pd.read_csv(DATA_DIR / \"test.csv\")\n","test_audio = DATA_DIR / \"test_audio\"\n","\n","\n","test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T05:52:25.060027Z","iopub.status.busy":"2022-12-02T05:52:25.059604Z","iopub.status.idle":"2022-12-02T05:52:25.401740Z","shell.execute_reply":"2022-12-02T05:52:25.400225Z","shell.execute_reply.started":"2022-12-02T05:52:25.059994Z"},"trusted":true},"outputs":[],"source":["sub = pd.read_csv(\"../input/birdsong-recognition/sample_submission.csv\")\n","sub.to_csv(\"submission.csv\", index=False)  # this will be overwritten if everything goes well"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T05:52:25.406269Z","iopub.status.busy":"2022-12-02T05:52:25.405744Z","iopub.status.idle":"2022-12-02T05:52:25.993847Z","shell.execute_reply":"2022-12-02T05:52:25.992702Z","shell.execute_reply.started":"2022-12-02T05:52:25.406228Z"},"trusted":true},"outputs":[],"source":["# load the training dataset \n","TRAIN = Path('/kaggle/input/birdsong-recognition/train.csv')\n","train = pd.read_csv(TRAIN)\n","train.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T05:52:25.996603Z","iopub.status.busy":"2022-12-02T05:52:25.996263Z","iopub.status.idle":"2022-12-02T05:52:26.051137Z","shell.execute_reply":"2022-12-02T05:52:26.050102Z","shell.execute_reply.started":"2022-12-02T05:52:25.996569Z"},"trusted":true},"outputs":[],"source":["train.info()"]},{"cell_type":"markdown","metadata":{},"source":["## Define Model"]},{"cell_type":"markdown","metadata":{},"source":["### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T06:34:46.964438Z","iopub.status.busy":"2022-12-02T06:34:46.964023Z","iopub.status.idle":"2022-12-02T06:34:46.977964Z","shell.execute_reply":"2022-12-02T06:34:46.976906Z","shell.execute_reply.started":"2022-12-02T06:34:46.964407Z"},"trusted":true},"outputs":[],"source":["class ResNet(nn.Module):\n","    def __init__(self, pretrained=False, num_classes=264):\n","        super().__init__()\n","        base_model = models.resnet50(\n","            pretrained=pretrained)\n","        layers = list(base_model.children())[:-2]\n","        layers.append(nn.AdaptiveMaxPool2d(1))\n","        self.encoder = nn.Sequential(*layers)\n","\n","        in_features = base_model.fc.in_features\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(in_features, 1024), nn.ReLU(), nn.Dropout(p=0.2),\n","            nn.Linear(1024, 1024), nn.ReLU(), nn.Dropout(p=0.2),\n","            nn.Linear(1024, num_classes))\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","        x = self.encoder(x)\n","        x = torch.flatten(x, start_dim = 1)  # batch_size, dim \n","        x = self.classifier(x)\n","        multiclass_proba = F.softmax(x, dim=1)\n","        multilabel_proba = F.sigmoid(x)\n","        return multilabel_proba\n","#         return {\n","#             \"logits\": x,\n","#             \"multiclass_proba\": multiclass_proba,\n","#             \"multilabel_proba\": multilabel_proba\n","#         }"]},{"cell_type":"markdown","metadata":{},"source":["## Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T06:32:40.505232Z","iopub.status.busy":"2022-12-02T06:32:40.504811Z","iopub.status.idle":"2022-12-02T06:32:40.511378Z","shell.execute_reply":"2022-12-02T06:32:40.510343Z","shell.execute_reply.started":"2022-12-02T06:32:40.505198Z"},"trusted":true},"outputs":[],"source":["weights_path = \"../input/birdcall-resnet50-init-weights/best.pth\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T06:32:42.100075Z","iopub.status.busy":"2022-12-02T06:32:42.099478Z","iopub.status.idle":"2022-12-02T06:32:42.131103Z","shell.execute_reply":"2022-12-02T06:32:42.129599Z","shell.execute_reply.started":"2022-12-02T06:32:42.100023Z"},"trusted":true},"outputs":[],"source":["BIRD_CODE = {\n","    'aldfly': 0, 'ameavo': 1, 'amebit': 2, 'amecro': 3, 'amegfi': 4,\n","    'amekes': 5, 'amepip': 6, 'amered': 7, 'amerob': 8, 'amewig': 9,\n","    'amewoo': 10, 'amtspa': 11, 'annhum': 12, 'astfly': 13, 'baisan': 14,\n","    'baleag': 15, 'balori': 16, 'banswa': 17, 'barswa': 18, 'bawwar': 19,\n","    'belkin1': 20, 'belspa2': 21, 'bewwre': 22, 'bkbcuc': 23, 'bkbmag1': 24,\n","    'bkbwar': 25, 'bkcchi': 26, 'bkchum': 27, 'bkhgro': 28, 'bkpwar': 29,\n","    'bktspa': 30, 'blkpho': 31, 'blugrb1': 32, 'blujay': 33, 'bnhcow': 34,\n","    'boboli': 35, 'bongul': 36, 'brdowl': 37, 'brebla': 38, 'brespa': 39,\n","    'brncre': 40, 'brnthr': 41, 'brthum': 42, 'brwhaw': 43, 'btbwar': 44,\n","    'btnwar': 45, 'btywar': 46, 'buffle': 47, 'buggna': 48, 'buhvir': 49,\n","    'bulori': 50, 'bushti': 51, 'buwtea': 52, 'buwwar': 53, 'cacwre': 54,\n","    'calgul': 55, 'calqua': 56, 'camwar': 57, 'cangoo': 58, 'canwar': 59,\n","    'canwre': 60, 'carwre': 61, 'casfin': 62, 'caster1': 63, 'casvir': 64,\n","    'cedwax': 65, 'chispa': 66, 'chiswi': 67, 'chswar': 68, 'chukar': 69,\n","    'clanut': 70, 'cliswa': 71, 'comgol': 72, 'comgra': 73, 'comloo': 74,\n","    'commer': 75, 'comnig': 76, 'comrav': 77, 'comred': 78, 'comter': 79,\n","    'comyel': 80, 'coohaw': 81, 'coshum': 82, 'cowscj1': 83, 'daejun': 84,\n","    'doccor': 85, 'dowwoo': 86, 'dusfly': 87, 'eargre': 88, 'easblu': 89,\n","    'easkin': 90, 'easmea': 91, 'easpho': 92, 'eastow': 93, 'eawpew': 94,\n","    'eucdov': 95, 'eursta': 96, 'evegro': 97, 'fiespa': 98, 'fiscro': 99,\n","    'foxspa': 100, 'gadwal': 101, 'gcrfin': 102, 'gnttow': 103, 'gnwtea': 104,\n","    'gockin': 105, 'gocspa': 106, 'goleag': 107, 'grbher3': 108, 'grcfly': 109,\n","    'greegr': 110, 'greroa': 111, 'greyel': 112, 'grhowl': 113, 'grnher': 114,\n","    'grtgra': 115, 'grycat': 116, 'gryfly': 117, 'haiwoo': 118, 'hamfly': 119,\n","    'hergul': 120, 'herthr': 121, 'hoomer': 122, 'hoowar': 123, 'horgre': 124,\n","    'horlar': 125, 'houfin': 126, 'houspa': 127, 'houwre': 128, 'indbun': 129,\n","    'juntit1': 130, 'killde': 131, 'labwoo': 132, 'larspa': 133, 'lazbun': 134,\n","    'leabit': 135, 'leafly': 136, 'leasan': 137, 'lecthr': 138, 'lesgol': 139,\n","    'lesnig': 140, 'lesyel': 141, 'lewwoo': 142, 'linspa': 143, 'lobcur': 144,\n","    'lobdow': 145, 'logshr': 146, 'lotduc': 147, 'louwat': 148, 'macwar': 149,\n","    'magwar': 150, 'mallar3': 151, 'marwre': 152, 'merlin': 153, 'moublu': 154,\n","    'mouchi': 155, 'moudov': 156, 'norcar': 157, 'norfli': 158, 'norhar2': 159,\n","    'normoc': 160, 'norpar': 161, 'norpin': 162, 'norsho': 163, 'norwat': 164,\n","    'nrwswa': 165, 'nutwoo': 166, 'olsfly': 167, 'orcwar': 168, 'osprey': 169,\n","    'ovenbi1': 170, 'palwar': 171, 'pasfly': 172, 'pecsan': 173, 'perfal': 174,\n","    'phaino': 175, 'pibgre': 176, 'pilwoo': 177, 'pingro': 178, 'pinjay': 179,\n","    'pinsis': 180, 'pinwar': 181, 'plsvir': 182, 'prawar': 183, 'purfin': 184,\n","    'pygnut': 185, 'rebmer': 186, 'rebnut': 187, 'rebsap': 188, 'rebwoo': 189,\n","    'redcro': 190, 'redhea': 191, 'reevir1': 192, 'renpha': 193, 'reshaw': 194,\n","    'rethaw': 195, 'rewbla': 196, 'ribgul': 197, 'rinduc': 198, 'robgro': 199,\n","    'rocpig': 200, 'rocwre': 201, 'rthhum': 202, 'ruckin': 203, 'rudduc': 204,\n","    'rufgro': 205, 'rufhum': 206, 'rusbla': 207, 'sagspa1': 208, 'sagthr': 209,\n","    'savspa': 210, 'saypho': 211, 'scatan': 212, 'scoori': 213, 'semplo': 214,\n","    'semsan': 215, 'sheowl': 216, 'shshaw': 217, 'snobun': 218, 'snogoo': 219,\n","    'solsan': 220, 'sonspa': 221, 'sora': 222, 'sposan': 223, 'spotow': 224,\n","    'stejay': 225, 'swahaw': 226, 'swaspa': 227, 'swathr': 228, 'treswa': 229,\n","    'truswa': 230, 'tuftit': 231, 'tunswa': 232, 'veery': 233, 'vesspa': 234,\n","    'vigswa': 235, 'warvir': 236, 'wesblu': 237, 'wesgre': 238, 'weskin': 239,\n","    'wesmea': 240, 'wessan': 241, 'westan': 242, 'wewpew': 243, 'whbnut': 244,\n","    'whcspa': 245, 'whfibi': 246, 'whtspa': 247, 'whtswi': 248, 'wilfly': 249,\n","    'wilsni1': 250, 'wiltur': 251, 'winwre3': 252, 'wlswar': 253, 'wooduc': 254,\n","    'wooscj2': 255, 'woothr': 256, 'y00475': 257, 'yebfly': 258, 'yebsap': 259,\n","    'yehbla': 260, 'yelwar': 261, 'yerwar': 262, 'yetvir': 263\n","}\n","\n","INV_BIRD_CODE = {v: k for k, v in BIRD_CODE.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T06:32:42.831310Z","iopub.status.busy":"2022-12-02T06:32:42.830897Z","iopub.status.idle":"2022-12-02T06:32:43.155086Z","shell.execute_reply":"2022-12-02T06:32:43.154218Z","shell.execute_reply.started":"2022-12-02T06:32:42.831271Z"},"trusted":true},"outputs":[],"source":["# find sample rate of trainin dataset \n","train_csv = pd.read_csv('/kaggle/input/birdsong-recognition/train.csv')\n","train_csv.head()  # ebidr_code filename"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T06:32:43.358315Z","iopub.status.busy":"2022-12-02T06:32:43.357722Z","iopub.status.idle":"2022-12-02T06:32:43.398060Z","shell.execute_reply":"2022-12-02T06:32:43.397034Z","shell.execute_reply.started":"2022-12-02T06:32:43.358271Z"},"trusted":true},"outputs":[],"source":["train_csv['full_path'] = '/kaggle/input/birdsong-recognition/train_audio/'+ train_csv['ebird_code']+ '/'+ train_csv['filename']\n","astfly = train_csv[train_csv['ebird_code'] == 'astfly'].sample(1, set_seed())['full_path'].values[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T06:32:43.985346Z","iopub.status.busy":"2022-12-02T06:32:43.984798Z","iopub.status.idle":"2022-12-02T06:32:43.991159Z","shell.execute_reply":"2022-12-02T06:32:43.990128Z","shell.execute_reply.started":"2022-12-02T06:32:43.985309Z"},"trusted":true},"outputs":[],"source":["train_csv.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T06:32:45.812569Z","iopub.status.busy":"2022-12-02T06:32:45.812176Z","iopub.status.idle":"2022-12-02T06:33:19.369024Z","shell.execute_reply":"2022-12-02T06:33:19.367656Z","shell.execute_reply.started":"2022-12-02T06:32:45.812537Z"},"trusted":true},"outputs":[],"source":["import torchaudio\n","import torchaudio.transforms as transforms\n","# get_sample_rate = lambda x: torchaudio.load(x)[-1]\n","# train_csv['sample_rate'] = train_csv['full_path'].apply(lambda x: torchaudio.load(x)[1] if torchaudio.load(x) else None)\n","# train_csv.head()\n","sample_example = []\n","\n","for index in random.sample(range(0, train_csv.shape[0]), 100):\n","    _, sample_rate = torchaudio.load(train_csv['full_path'][index])\n","    sample_example.append(sample_rate)\n","print(np.average(sample_example)) # 48000"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T06:33:19.372349Z","iopub.status.busy":"2022-12-02T06:33:19.371841Z","iopub.status.idle":"2022-12-02T06:33:19.396271Z","shell.execute_reply":"2022-12-02T06:33:19.395009Z","shell.execute_reply.started":"2022-12-02T06:33:19.372289Z"},"trusted":true},"outputs":[],"source":["class AudioUtil():\n","#     def __init__(self)\n","\n","    def open(self, file_path):\n","        return torchaudio.load(file_path)\n","\n","    def resample(self, aud, newsr):\n","        sig, sr = aud\n","        if sr == newsr:\n","            return aud \n","        num_channels = sig.shape[0]\n","        output = []\n","        resig = torchaudio.transforms.Resample(sr, newsr)(sig)\n","        if num_channels == 1:\n","            \n","            return torch.cat([resig, resig, resig]), newsr\n","        elif num_channels == 2:\n","            return torch.cat([resig, torch.average(resig, axis = 0).reshape(1, -1)]), newsr\n","    \n","    def pad_trunc(self, aud, max_s):  # max_s = 5\n","        sig, sr = aud\n","        num_rows, sig_len = sig.shape\n","        max_len = sr * max_s\n","\n","        if (sig_len > max_len):\n","          # Truncate the signal to the given length\n","            sig = sig[:,:max_len]\n","\n","        elif (sig_len < max_len):\n","          # Length of padding to add at the beginning and end of the signal\n","\n","            pad_end_len = max_len - sig_len\n","            pad_end = torch.zeros((num_rows, pad_end_len))\n","            sig = torch.cat((sig, pad_end), 1)\n","          \n","        return (sig, sr)\n","    \n","    \n","\n","    def time_shift(self, aud, shift_limit):\n","        sig, sr = aud\n","        _, sig_len = sig.shape\n","        shift_amt = int(random.random() * shift_limit * sig_len)\n","        return (sig.roll(shift_amt), sr)\n","    \n","\n","    def spectro_gram(self, aud, n_mels=64, n_fft=1024, hop_len=None):\n","        sig, sr = aud\n","        top_db = 80\n","\n","        # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n","        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n","\n","        # Convert to decibels\n","        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n","        return (spec)\n","\n","    def spectro_augment(self, spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n","        _, n_mels, n_steps = spec.shape\n","        mask_value = spec.mean()\n","        aug_spec = spec\n","\n","        freq_mask_param = max_mask_pct * n_mels\n","        for _ in range(n_freq_masks):\n","            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n","\n","        time_mask_param = max_mask_pct * n_steps\n","        for _ in range(n_time_masks):\n","            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n","\n","        return aug_spec\n","            "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T06:33:19.398799Z","iopub.status.busy":"2022-12-02T06:33:19.398096Z","iopub.status.idle":"2022-12-02T06:33:19.417871Z","shell.execute_reply":"2022-12-02T06:33:19.416975Z","shell.execute_reply.started":"2022-12-02T06:33:19.398747Z"},"trusted":true},"outputs":[],"source":["class TrainDataset(data.Dataset):  # train size = 3, 224, x\n","    def __init__(self, df, img_size = 224, sr = 32000, duration = 5):\n","        self.df = df \n","        self.img_size = img_size \n","        self.melspectrogram_parameters = melspectrogram_parameters\n","        self.sr = sr\n","        self.duration = duration\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx: int):\n","\n","        sample = self.df.loc[idx, :]\n","        file_path = '/kaggle/input/birdsong-recognition/train_audio/' + sample['ebird_code'] + '/' + sample['filename']\n","\n","        aud = AudioUtil().open(file_path)\n","        reaud = AudioUtil().resample(aud, self.sr)  # and channel into 3 \n","        dur_aud = AudioUtil().pad_trunc(reaud, self.duration)\n","        shift_aud = AudioUtil().time_shift(dur_aud, shift_limit = 0.5)\n","        sgram = AudioUtil().spectro_gram(shift_aud, n_mels=self.img_size, n_fft=1024, hop_len=None)  # size would be 3, 244, 313; why \n","        aug_sgram = AudioUtil().spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n","\n","        return aug_sgram, idx\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class TestDataset(data.Dataset):  # train size = 3, 224, x\n","    def __init__(self, df, img_size = 224, sr = 32000, duration = 5):\n","        self.df = df \n","        self.img_size = img_size \n","        self.melspectrogram_parameters = melspectrogram_parameters\n","        self.sr = sr\n","        self.duration = duration\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx: int):\n","\n","        sample = self.df.loc[idx, :]\n","        site = sample.site \n","        audio_id = sample.audio_id\n","        duration = sample.seconds\n","        file_path = '/kaggle/input/birdcall-check/test_audio/' + audio_id\n","        aud = AudioUtil().open(file_path)\n","        reaud = AudioUtil().resample(aud, self.sr)  # and channel into 3 \n","        \n","        if site == 'site_3':\n","            images = []\n","            start = 0\n","            duration = 5\n","            while start < duration:\n","                end = 5 * self.sr + start\n","\n","                if end > duration:\n","                    reaud1 = reaud[:, start:]\n","                    reaud1 = AudioUtil.pad_trunc(reaud1, self.duration)\n","                else:\n","                    reaud1 = reaud[:, start:end]\n","                \n","                \n","                sgram = AudioUtil().spectro_gram(reaud1, n_mels=self.img_size, n_fft=1024, hop_len=None)  # size would be 3, 244, 313\n","                print(sgram.shape)\n","                images.append(sgram)\n","                start = end \n","            return np.array(images), audio_id, site\n","\n","        else:\n","            end_seconds = int(sample.seconds)\n","            start_seconds = int(end_seconds - 5)\n","            \n","            start_index = self.sr * start_seconds\n","            end_index = self.sr * end_seconds\n","            dur_aud = reaud[:, start_index:end_index]\n","            sgram = AudioUtil().spectro_gram(dur_aud, n_mels=self.img_size, n_fft=1024, hop_len=None)  # size would be 3, 244, 313\n","            return sgram, audio_id, site\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T06:33:19.419987Z","iopub.status.busy":"2022-12-02T06:33:19.419323Z","iopub.status.idle":"2022-12-02T06:33:19.439903Z","shell.execute_reply":"2022-12-02T06:33:19.438426Z","shell.execute_reply.started":"2022-12-02T06:33:19.419938Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import random_split\n","my_ds = TrainDataset(df = train_csv)\n","# mydataset.__getitem__(10000)[0].shape\n","\n","num_items = len(my_ds)\n","num_train = round(num_items * 0.8)\n","num_valid = num_items - num_train\n","train_ds, valid_ds = random_split(my_ds, [num_train, num_valid])\n","\n","\n","train_dl = torch.utils.data.DataLoader(train_ds, batch_size = 16, shuffle = True)\n","valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size = 16, shuffle = False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T06:33:19.442826Z","iopub.status.busy":"2022-12-02T06:33:19.442417Z","iopub.status.idle":"2022-12-02T06:33:19.455094Z","shell.execute_reply":"2022-12-02T06:33:19.453625Z","shell.execute_reply.started":"2022-12-02T06:33:19.442788Z"},"trusted":true},"outputs":[],"source":["def valid(model, val_dl):\n","    correct_prediction = 0\n","    total_prediction = 0\n","\n","    # Disable gradient updates\n","    with torch.no_grad():\n","        for data in val_dl:\n","            # Get the input features and target labels, and put them on the GPU\n","            inputs, labels = data[0].to(device), data[1].to(device)\n","\n","            # Normalize the inputs\n","            inputs_m, inputs_s = inputs.mean(), inputs.std()\n","            inputs = (inputs - inputs_m) / inputs_s\n","\n","            # Get predictions\n","            outputs = model(inputs)\n","\n","            # Get the predicted class with the highest score\n","            _, prediction = torch.max(outputs,1)\n","            # Count of predictions that matched the target label\n","            correct_prediction += (prediction == labels).sum().item()\n","            total_prediction += prediction.shape[0]\n","\n","    acc = correct_prediction/total_prediction\n","    print(f'Validation Accuracy: {acc:.2f}, Total items: {total_prediction}')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T06:34:53.693409Z","iopub.status.busy":"2022-12-02T06:34:53.692743Z","iopub.status.idle":"2022-12-02T06:35:14.268079Z","shell.execute_reply":"2022-12-02T06:35:14.265451Z","shell.execute_reply.started":"2022-12-02T06:34:53.693371Z"},"trusted":true},"outputs":[],"source":["## start train model \n","\n","def training(model, train_dl, valid_dl, num_epochs):\n","    # Loss Function, Optimizer and Scheduler\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n","    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n","                                                steps_per_epoch=int(len(train_dl)),\n","                                                epochs=num_epochs,\n","                                                anneal_strategy='linear')\n","\n","    # Repeat for each epoch\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        correct_prediction = 0\n","        total_prediction = 0\n","\n","        # Repeat for each batch in the training set\n","        for i, data in enumerate(train_dl):\n","            # Get the input features and target labels, and put them on the GPU\n","            inputs, labels = data[0].to(device), data[1].to(device)\n","\n","            # Normalize the inputs\n","            inputs_m, inputs_s = inputs.mean(), inputs.std()\n","            inputs = (inputs - inputs_m) / inputs_s\n","\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","            # Keep stats for Loss and Accuracy\n","            running_loss += loss.item()\n","\n","            # Get the predicted class with the highest score\n","            _, prediction = torch.max(outputs,1)\n","            # Count of predictions that matched the target label\n","            correct_prediction += (prediction == labels).sum().item()\n","            total_prediction += prediction.shape[0]\n","\n","    #         if i % 10 == 0:    # print every 10 mini-batches\n","    #             print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n","                # Run inference on trained model with the validation set\n","\n","\n","        # Print stats at the end of the epoch\n","        num_batches = len(train_dl)\n","        avg_loss = running_loss / num_batches\n","        acc = correct_prediction/total_prediction\n","        print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n","        if epoch % 10 == 0:\n","            valid(model, val_dl)\n","\n","        print('Finished Training')\n","\n","num_epochs=20   # Just for demo, adjust this higher.\n","myModel = ResNet(pretrained=True)\n","training(myModel, train_dl, valid_dl, num_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def prediction(test_df: pd.DataFrame,\n","               test_audio: Path,\n","               model_config: dict,\n","               mel_params: dict,\n","               weights_path: str,\n","               threshold=0.5):\n","    model = get_model(model_config, weights_path)\n","    unique_audio_id = test_df.audio_id.unique()\n","\n","    warnings.filterwarnings(\"ignore\")\n","    prediction_dfs = []\n","    for audio_id in unique_audio_id:        \n","        test_df_for_audio_id = test_df.query(\n","            f\"audio_id == '{audio_id}'\").reset_index(drop=True)\n","        with timer(f\"Prediction on {audio_id}\", logger):\n","            prediction_dict = prediction_for_clip(test_df_for_audio_id,\n","                                                  model=model,\n","                                                  mel_params=mel_params,\n","                                                  threshold=threshold)\n","        row_id = list(prediction_dict.keys())\n","        birds = list(prediction_dict.values())\n","        prediction_df = pd.DataFrame({\n","            \"row_id\": row_id,\n","            \"birds\": birds\n","        })\n","        prediction_dfs.append(prediction_df)\n","    \n","    prediction_df = pd.concat(prediction_dfs, axis=0, sort=False).reset_index(drop=True)\n","    return prediction_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T06:14:07.880050Z","iopub.status.busy":"2022-12-02T06:14:07.879229Z","iopub.status.idle":"2022-12-02T06:14:09.062819Z","shell.execute_reply":"2022-12-02T06:14:09.061632Z","shell.execute_reply.started":"2022-12-02T06:14:07.879991Z"},"trusted":true},"outputs":[],"source":["\n","        \n","def prediction_for_clip(test_df: pd.DataFrame, \n","                        model: ResNet, \n","                        mel_params: dict, \n","                        threshold=0.5):\n","\n","    dataset = TestDataset(df=test_df, \n","                          img_size=224,\n","                          melspectrogram_parameters=mel_params)\n","    loader = data.DataLoader(dataset, batch_size=1, shuffle=False)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    \n","    model.eval()\n","    prediction_dict = {}\n","    for image, row_id, site in progress_bar(loader):\n","                    # Normalize the inputs\n","        inputs_m, inputs_s = image.mean(), image.std()\n","        image = (image - inputs_m) / inputs_s\n","        site = site[0]\n","        row_id = row_id[0]\n","        if site in {\"site_1\", \"site_2\"}:\n","            image = image.to(device)\n","\n","            with torch.no_grad():\n","                prediction = model(image)\n","                proba = prediction[\"multilabel_proba\"].detach().cpu().numpy().reshape(-1)\n","\n","            events = proba >= threshold\n","            labels = np.argwhere(events).reshape(-1).tolist()\n","\n","        else:\n","            # to avoid prediction on large batch\n","            image = image.squeeze(0)\n","            batch_size = 16\n","            whole_size = image.size(0)\n","            if whole_size % batch_size == 0:\n","                n_iter = whole_size // batch_size\n","            else:\n","                n_iter = whole_size // batch_size + 1\n","                \n","            all_events = set()\n","            for batch_i in range(n_iter):\n","                batch = image[batch_i * batch_size:(batch_i + 1) * batch_size]\n","                if batch.ndim == 3:\n","                    batch = batch.unsqueeze(0)\n","\n","                batch = batch.to(device)\n","                with torch.no_grad():\n","                    prediction = model(batch)\n","                    proba = prediction[\"multilabel_proba\"].detach().cpu().numpy()\n","                    \n","                events = proba >= threshold\n","                for i in range(len(events)):\n","                    event = events[i, :]\n","                    labels = np.argwhere(event).reshape(-1).tolist()\n","                    for label in labels:\n","                        all_events.add(label)\n","                        \n","            labels = list(all_events)\n","        if len(labels) == 0:\n","            prediction_dict[row_id] = \"nocall\"\n","        else:\n","            labels_str_list = list(map(lambda x: INV_BIRD_CODE[x], labels))\n","            label_string = \" \".join(labels_str_list)\n","            prediction_dict[row_id] = label_string\n","    return prediction_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submission = prediction(test_df=test,\n","                        test_audio=test_audio,\n","                        model_config=model_config,\n","                        mel_params=melspectrogram_parameters,\n","                        weights_path=weights_path,\n","                        threshold=0.8)\n","submission.to_csv(\"submission.csv\", index=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Prediction loop"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T06:15:37.814692Z","iopub.status.busy":"2022-12-02T06:15:37.814303Z","iopub.status.idle":"2022-12-02T06:15:37.821102Z","shell.execute_reply":"2022-12-02T06:15:37.820113Z","shell.execute_reply.started":"2022-12-02T06:15:37.814661Z"},"trusted":true},"outputs":[],"source":["def get_model(config: dict, weights_path: str):\n","    model = ResNet(**config)\n","    checkpoint = torch.load(weights_path)\n","    model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    device = torch.device(\"cuda\")\n","    model.to(device)\n","    model.eval()\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}
